# -*- coding: utf-8 -*-
"""gradient_explanations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/devayani-kv/gradient_explanations/blob/main/gradient_explanations.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

pip install -q -U "tensorflow-text==2.8.*"

pip install -q tf-models-official==2.7.0

import os
import shutil
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

df = pd.read_csv('/content/waseem_v1.csv', header = None)
df

df.dropna(inplace = True)
df.reset_index(inplace = True)
df

df.drop('index', inplace = True, axis=1)
df

def fun(n):
  if n == 0.0:
    return 0
  else:
    return 1
df[2] = df[2].apply(fun)
df

from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(df[1],df[2], test_size = 0.2, random_state = 32)

np.shape(y_val)

train_df = df.iloc[0:8000, :]
val_df = df.iloc[8000:, :]
train_df.info()

pip install transformers

import transformers 
from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification,TFBertForSequenceClassification
from transformers import AutoTokenizer
from transformers import BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')

x_train, y_train = list(x_train.astype(str)), list(y_train)
type(x_train)

train_encodings = tokenizer(x_train, padding=True, truncation=True)
train_encodings

train_ds = tf.data.Dataset.from_tensor_slices((
      dict(train_encodings),
      y_train
  ))
train_ds = train_ds.batch(1)
train_ds

model = TFBertForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)

lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=5e-5,
    decay_steps=10000,
    decay_rate=0.9)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
model.compile(optimizer=optimizer,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=tf.metrics.SparseCategoricalAccuracy()
              )

model.fit(x=train_ds, epochs=3)

x_val, y_val = list(x_val.astype(str)), list(y_val)
val_encodings = tokenizer(x_val, padding=True, truncation=True)
val_ds = tf.data.Dataset.from_tensor_slices((
      dict(val_encodings),
      y_val
  ))
val_ds = val_ds.batch(1)
val_ds

loss, accuracy = model.evaluate(val_ds)



